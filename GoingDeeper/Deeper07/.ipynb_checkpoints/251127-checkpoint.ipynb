{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec0d5870-75bd-43a5-b375-9d3e6071f7ec",
   "metadata": {},
   "source": [
    "# Tokenizer 준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdea2ef6-9b97-4138-90fe-580308c10b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sentencepiece as spm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# torch version\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4475e269-1379-4190-8d25-5814559b8141",
   "metadata": {},
   "source": [
    "준비해 둔 한글 나무위키 코퍼스로부터 32000의 vocab_size를 갖는 sentencepiece 모델을 생성해 보겠습니다.\n",
    "\n",
    "BERT에 사용되는 [MASK], [SEP], [CLS] 등의 주요 특수문자가 vocab에 포함되어야 함에 주의해 주세요\n",
    "\n",
    "(선택) 직접 model 과 vocab 을 만들기 : cloud shell을 열어 아래 코드를 한줄씩 실행\n",
    "$ python\n",
    ">>> import sentencepiece as spm\n",
    ">>> import os\n",
    ">>> corpus_file = os.getenv('HOME')+'/work/bert_pretrain/data/kowiki.txt'\n",
    ">>> prefix = os.getenv('HOME')+'/work/bert_pretrain/models/ko_32000'\n",
    ">>> vocab_size = 32000\n",
    ">>> spm.SentencePieceTrainer.train(f\"--input={corpus_file} --model_prefix={prefix} --vocab_size={vocab_size + 7} --model_type=bpe --max_sentence_length=999999 --pad_id=0 --pad_piece=[PAD] --unk_id=1 --unk_piece=[UNK] --bos_id=2 --bos_piece=[BOS] --eos_id=3 --eos_piece=[EOS] --user_defined_symbols=[SEP],[CLS],[MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b327491-60d4-43f8-b4c3-4842972f69c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "corpus_file = '/home/jovyan/work/bert_pretrain/data/kowiki.txt'\n",
    "prefix = '/home/jovyan/work/bert_pretrain/data/kowiki.txt'\n",
    "vocab_size = 32000\n",
    "#센텐스피스를 직접 만들어보실 분들은 주석을 풀어 진행해보세요! \n",
    "#(LMS 환경에서는 시간이 오래 걸려요)\n",
    "#spm.SentencePieceTrainer.train(f\"--input={corpus_file} --model_prefix={prefix} --vocab_size={vocab_size + 7} --model_type=bpe --max_sentence_length=999999 --pad_id=0 --pad_piece=[PAD] --unk_id=1 --unk_piece=[UNK] --bos_id=2 --bos_piece=[BOS] --eos_id=3 --eos_piece=[EOS] --user_defined_symbols=[SEP],[CLS],[MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3a368b-6212-494b-96e3-bd4523f3485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dir = '/home/jovyan/work/bert_pretrain/data'\n",
    "\n",
    "# vocab loading\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(f\"{data_dir}/ko_32000.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554cbbb6-ff6a-4604-bfde-00f2225dbadb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
