{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83723832-a62b-4b93-8678-c4a7387f9f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install loralib\n",
    "!pip install trl\n",
    "!pip install accelerate\n",
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76d010a-38b6-44cd-9fe3-51083c3f71da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/airobotlab/KoChatGPT\n",
    "!cp -r /content/KoChatGPT/colossalai_ChatGPT_230319/chatgpt /content/chatgpt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2ee16f-4861-4384-878a-6ee0e078cd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "modifications = [\n",
    "    {\n",
    "        \"file\": \"chatgpt/trainer/callbacks/save_checkpoint.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 3, \"old\": \"from chatgpt.trainer.strategies import ColossalAIStrategy, Strategy\",\n",
    "             \"new\": \"from chatgpt.trainer.strategies import Strategy\"},\n",
    "            {\"line\": 71, \"old\": \"only_rank0 = not isinstance(self.strategy, ColossalAIStrategy)\",\n",
    "             \"new\": \"            only_rank0 = not isinstance(self.strategy)\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"chatgpt/trainer/strategies/__init__.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 1, \"old\": \"from .colossalai import ColossalAIStrategy\", \"new\": \"\"},  # 삭제\n",
    "            {\"line\": 5, \"old\": \"__all__ = ['Strategy', 'NaiveStrategy', 'DDPStrategy', 'ColossalAIStrategy']\",\n",
    "             \"new\": \"__all__ = ['Strategy', 'NaiveStrategy', 'DDPStrategy']\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"chatgpt/dataset/reward_dataset.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 3, \"old\": \"from tqdm import tqdm\", \"new\": \"from tqdm.notebook import tqdm\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"chatgpt/trainer/strategies/__init__.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 8, \"old\": \"from tqdm import tqdm\", \"new\": \"from tqdm.notebook import tqdm\"},\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"file\": \"chatgpt/dataset/reward_dataset.py\",\n",
    "        \"changes\": [\n",
    "            {\"line\": 8, \"old\": \"from tqdm import tqdm\", \"new\": \"from tqdm.notebook import tqdm\"},\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "def modify_file(file_path, changes):\n",
    "    \"\"\"파일에서 지정된 줄을 찾아 내용을 수정하는 함수\"\"\"\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"⚠️ 파일이 존재하지 않습니다: {file_path}\")\n",
    "        return\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    modified = False\n",
    "\n",
    "    for change in changes:\n",
    "        line_index = change[\"line\"]\n",
    "        if 0 <= line_index < len(lines):\n",
    "            if lines[line_index].strip() == change[\"old\"]:\n",
    "                lines[line_index] = change[\"new\"] + \"\\n\"\n",
    "                modified = True\n",
    "            else:\n",
    "                print(f\"⚠️ {file_path} 파일의 {change['line']}번째 줄이 예상과 다릅니다.\")\n",
    "                print(f\"   예상: {change['old']}\")\n",
    "                print(f\"   실제: {lines[line_index].strip()}\")\n",
    "\n",
    "    if modified:\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.writelines(lines)\n",
    "        print(f\"✅ 수정 완료: {file_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️ {file_path} 수정할 내용이 없습니다.\")\n",
    "\n",
    "for mod in modifications:\n",
    "    modify_file(mod[\"file\"], mod[\"changes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b378a512-13d6-412b-ac0f-36c6b5f47793",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "\n",
    "print(\"Torch version:{}\".format(torch.__version__)) # Torch version:1.12.1\n",
    "print(\"Cuda version: {}\".format(torch.version.cuda)) # Cuda version: 11.3\n",
    "print(\"transformers version: {}\".format(transformers.__version__)) # transformers 4.28.0\n",
    "print(\"GPU 사용 가능여부: {}\".format(torch.cuda.is_available()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46c130f-6506-429d-af93-22aeab6569e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee98763-9aa7-45fc-9152-8e3709d55d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우리가 사용할 모델의 토크나이저가 입력받아 처리할 수 있는 최대 토큰 수를 확인\n",
    "\n",
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bb692e-6b09-4648-b928-23c9f293219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kogpt-2는 어떻게 토크나이징을 하는지 잠시 확인\n",
    "model.config.n_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1150e2a6-257a-4ec1-b70b-74de045397ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_txt = \"바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7ee49a-4b71-4c0c-bdb4-ce58240c15bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(input_txt).tokens()\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c543d7-9ace-45b7-8871-3f486fac79bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 40\n",
    "pd.options.display.max_rows = 60\n",
    "df = pd.DataFrame([tokens, input_ids[0]], index=[\"kogpt-2_tokens\", \"Input_IDs\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14a910f-7a31-487d-895b-c99e03973881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코딩 성능 확인\n",
    "\n",
    "max_length=128\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfb31c3-a67a-45cb-9c9b-e4a8f18f3a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=10, no_repeat_ngram_size=2,do_sample=False)\n",
    "print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2cd190-d1b5-4017-8b58-e97b5f44a563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 확인\n",
    "\n",
    "import json\n",
    "data_path_1_SFT = 'KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl'\n",
    "with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2490d6d-e806-4ad5-954f-e96e6c749dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=7, no_repeat_ngram_size=2,\n",
    "                             do_sample=True, temperature=2.0, top_k=50)\n",
    "print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6079efc7-0071-476f-96fb-ce0dae30a536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_p 샘플링 기법\n",
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=7, no_repeat_ngram_size=2,\n",
    "                             do_sample=True, top_p=0.90)\n",
    "print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b2617c-cbec-4181-a894-20cf9e70ad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 확인\n",
    "import json\n",
    "data_path_1_SFT = 'KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl'\n",
    "with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9acc5f2-e13a-4d06-a6fa-490444ff291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm에 사용할 데이터셋확인\n",
    "data_path_2_RM = 'KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl'\n",
    "with open(data_path_2_RM, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264a2cd2-cbb5-45e9-843b-1386a1c5da7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppo학습에 쓰일 데이터 \n",
    "data_path_3_PPO = 'KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl'\n",
    "with open(data_path_3_PPO, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62891835-fabc-4f80-98a2-f5044ae0fafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Sequence\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0f4d31-7d0f-4e13-b650-522802f97402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델과 토크나이저를 불러오기\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7a5aa1-1815-4e62-b03d-e7cb41d6e2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 인퍼런스 단계에서 사용할 prompt 딕셔너리 템플릿과 SFT 데이터셋 클래스를 정의\n",
    "\n",
    "\n",
    "class SFT_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "\n",
    "        pattern_instruction = 'prompt'  # instruction\n",
    "        pattern_output = 'completion'  # response\n",
    "\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            list_data_dict = json.load(json_file)\n",
    "\n",
    "        PROMPT_DICT = {\n",
    "            \"prompt_input\": (\n",
    "                \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "        prompt_input = PROMPT_DICT[\"prompt_input\"]\n",
    "\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            tmp = prompt_input.format_map(example)\n",
    "            sources.append(tmp)\n",
    "\n",
    "        targets = []\n",
    "        for example in list_data_dict:\n",
    "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
    "\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = -100\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n",
    "\n",
    "\n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8fa6f6-d100-4e53-8926-e99c47735305",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value= -100)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9920242-d211-4567-9b09-23955ff603f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT_dataset 클래스를 사용해 훈련셋을 만들고 data collator 인스턴스를 만들기\n",
    "train_dataset = SFT_dataset(data_path_1_SFT='KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl', tokenizer=tokenizer)\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "print('input : %s'%train_dataset.input_ids[0])\n",
    "print('output: %s'%train_dataset.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1879a38-0003-4267-b96b-ebd3de6e5303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def verify_tokenization(dataset, tokenizer):\n",
    "    print(\"## Tokenization Verification Start ##\\n\")\n",
    "\n",
    "    # 1. 첫 번째 데이터 샘플 가져오기\n",
    "    # dataset[0]은 {'input_ids': tensor, 'labels': tensor} 형태입니다.\n",
    "    sample = dataset[0]\n",
    "    input_ids = sample['input_ids']\n",
    "    labels = sample['labels']\n",
    "\n",
    "    # 2. input_ids 디코딩 (전체 문장 확인)\n",
    "    # input_ids는 [Instruction + Response]가 모두 포함된 상태입니다.\n",
    "    decoded_input = tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "    print(f\"1. [Input_ids] Full Text (Instruction + Response):\\n{'-'*40}\")\n",
    "    print(decoded_input)\n",
    "    print(f\"{'-'*40}\\n\")\n",
    "\n",
    "    # 3. labels 디코딩 (학습 대상 확인)\n",
    "    # labels에서 -100은 Loss 계산에서 제외되는 부분(Instruction)입니다.\n",
    "    # -100이 아닌 부분(Response)만 추출하여 디코딩합니다.\n",
    "    valid_labels = [token_id for token_id in labels if token_id != -100]\n",
    "    decoded_labels = tokenizer.decode(valid_labels, skip_special_tokens=False)\n",
    "\n",
    "    print(f\"2. [Labels] Training Target (Response Only):\\n{'-'*40}\")\n",
    "    print(decoded_labels)\n",
    "    print(f\"{'-'*40}\\n\")\n",
    "\n",
    "    # 4. 토큰별 매핑 확인 (선택 사항: 마스킹 위치 시각화)\n",
    "    print(\"3. [Detailed Mapping] Token vs Label Masking (First 20 tokens):\")\n",
    "    for i, (inp, lbl) in enumerate(zip(input_ids[:20], labels[:20])):\n",
    "        inp_token = tokenizer.decode([inp])\n",
    "        lbl_status = \"MASKED (-100)\" if lbl == -100 else tokenizer.decode([lbl])\n",
    "        print(f\"Idx {i:02d} | Input: {inp_token:<10} | Label: {lbl_status}\")\n",
    "\n",
    "    print(\"\\n## Verification Done ##\")\n",
    "\n",
    "# --- 사용 예시 ---\n",
    "# 데이터셋 로드 (가정)\n",
    "# train_dataset = SFT_dataset(data_path_1_SFT='your_data.json', tokenizer=tokenizer)\n",
    "\n",
    "# 함수 실행\n",
    "# verify_tokenization(train_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6de4e37-d2d7-4ddd-bb46-68bfe9476def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련을 위한 마지막 단계로 Training arguments를 사용해 trainer 클래스를 정의\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"test\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=5,\n",
    "    prediction_loss_only=True,\n",
    "    fp16 = True\n",
    "    )\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d96562-3970-4cd9-8268-542776b8a747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT 훈련을 진행\n",
    "trainer.train()\n",
    "model.save_pretrained('models/output_1_SFT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74e6fc0-5b31-4031-906e-52b7b9eba720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 문장 생성 능력을 확인하기 위해 빠르게 허깅페이스의 pipleline 클래스를 사용하여 generator를 만들어보겠습니다.\n",
    "\n",
    "generator = transformers.pipeline('text-generation', model='models/output_1_SFT', tokenizer=tokenizer)\n",
    "\n",
    "generation_args = dict(\n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=375, # \\n\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = ['불고기용 고기 한우에요?',\n",
    "               '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "               '시카고 오헤어 국제공항은 어디에 있어?',\n",
    "               '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = generator(list_prompt, **generation_args)\n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print()\n",
    "    print((result[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cc1f74-ecc1-41ca-8355-da0ea4a3afe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 관리를 위해 캐시를 비우기\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73013920-6af8-46ae-979e-a315e772a967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatgpt.dataset import RewardDataset\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from chatgpt.trainer.rm import RewardModelTrainer\n",
    "\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cf7685-b67f-479a-a6f5-88732bf19565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPTRM_custom 이라는 이름으로 클래스를 선언\n",
    "\n",
    "class GPTRM_custom(RewardModel):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pretrained: Optional[str] = None,\n",
    "                 config: Optional[GPT2Config] = None,\n",
    "                 checkpoint: bool = False,\n",
    "                 lora_rank: int = 0,\n",
    "                 lora_train_bias: str = 'none',\n",
    "                 tokenizer=None) -> None:\n",
    "        if pretrained is not None:\n",
    "            model = GPT2Model.from_pretrained(pretrained)\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        elif config is not None:\n",
    "            model = GPT2Model(config)\n",
    "        else:\n",
    "            model = GPT2Model(GPT2Config())\n",
    "        if checkpoint:\n",
    "            model.gradient_checkpointing_enable()\n",
    "\n",
    "        value_head = nn.Linear(model.config.n_embd, 1)\n",
    "        super().__init__(model, value_head, lora_rank, lora_train_bias)\n",
    "\n",
    "        if pretrained is not None:\n",
    "            self.model = model\n",
    "            self.pretrained = pretrained\n",
    "\n",
    "\n",
    "    def save_pretrained(self, dir):\n",
    "        if self.pretrained is not None:\n",
    "            self.model.save_pretrained(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f7dcb0-1725-4b27-ab19-d95a6fc0461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "with NaiveStrategy().model_init_context():\n",
    "        model = GPTRM_custom(pretrained='skt/kogpt2-base-v2', lora_rank=0, tokenizer=tokenizer).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21367ce1-125a-4a22-b40b-7a0920fc116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RM을 훈련시킬 때 사용할 ranking dataset\n",
    "with open('KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "total_data_ranking2chosen = []\n",
    "for tmp in list_data_dict:\n",
    "    one_data_ranking2chosen = []\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "\n",
    "\n",
    "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
    "\n",
    "print('before data num: %d'%(len(list_data_dict)))\n",
    "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
    "print('data example: \\n%s'%total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22db71f-2423-42a7-b3ef-444fb70c7a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairWiseLoss(nn.Module):\n",
    "\n",
    "    def forward(self, chosen_reward: torch.Tensor, reject_reward: torch.Tensor) -> torch.Tensor:\n",
    "        probs = torch.sigmoid(chosen_reward - reject_reward)\n",
    "        log_probs = torch.log(probs)\n",
    "        loss = -log_probs.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f11160-debe-4caa-8634-e8bed334942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_ranking2chosen = []\n",
    "\n",
    "for tmp in list_data_dict:\n",
    "     prompt = tmp['prompt']\n",
    "     ranking = tmp['ranking']\n",
    "\n",
    "     for index in range(1, len(ranking)):\n",
    "         n = ranking[0]\n",
    "         m = ranking[index]\n",
    "\n",
    "\n",
    "         data = {\n",
    "             'prompt': prompt,\n",
    "             'chosen': tmp['completion_{}'.format(n)],\n",
    "             'rejected': tmp['completion_{}'.format(m)]\n",
    "         }\n",
    "\n",
    "         total_data_ranking2chosen.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a53559-a185-4026-ba8d-6c4e461b3734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranking dataset을 shuffle한 후 훈련셋을 만들\n",
    "import random\n",
    "random.seed(230319)\n",
    "random.shuffle(total_data_ranking2chosen)\n",
    "print(total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e36fbe-da1b-4cfd-975a-dfa9c626087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = total_data_ranking2chosen[:1000]\n",
    "eval_data = total_data_ranking2chosen[1000:1200]\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(eval_data))\n",
    "\n",
    "train_dataset = RewardDataset(train_data, tokenizer, 512)\n",
    "eval_dataset = RewardDataset(eval_data, tokenizer, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b224abb-07cb-4569-aa47-c556b4dd7248",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "print('#'*70)\n",
    "print('## prompt ##')\n",
    "print(train_data[idx]['prompt'])\n",
    "print('#'*70)\n",
    "print('## chosen ##')\n",
    "print(train_data[idx]['chosen'])\n",
    "print('#'*70)\n",
    "print('## rejected ##')\n",
    "print(train_data[idx]['rejected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9721a3cb-a019-460a-bf05-a049278a3a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm 학습\n",
    "\n",
    "trainer = RewardModelTrainer(model=model,\n",
    "                             strategy=NaiveStrategy(),\n",
    "                             optim=torch.optim.Adam(model.parameters(), lr=5e-5),\n",
    "                             train_dataset=train_dataset,\n",
    "                             eval_dataset=eval_dataset,\n",
    "                             batch_size=4,\n",
    "                             max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae30294-e1d7-40f6-a5af-98f5f6b8b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RM 학습이 잘 되었는지 확인해보기 위해 임의의 문장을 입력한 후 적절한 reward score를 출력하는지 살펴보도록 하겠습니다.\n",
    "def inference_RM(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').cuda()\n",
    "    output = model(input_ids)\n",
    "    output_reward = output.cpu().detach().numpy()[0]\n",
    "\n",
    "    print('input: %s\\nreward score: %.1f'%(input_text, output_reward))\n",
    "\n",
    "    return output_reward\n",
    "\n",
    "input_text = '인공지능은 똥멍청이 입니다'\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59091011-8b2c-4225-b93d-dc84276e0d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = '인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다.'\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3ee2fa-560f-4333-a152-57c7a802ddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다. AI는 현대적인 컴퓨팅 혁신에서 중추적인 역할을 하며 개인과 비즈니스의 가치를 창출합니다. 예를 들어 광학 문자 인식(OCR)은 AI를 사용해 이미지 및 문서에서 텍스트 및 데이터를 추출하고, 구조화되지 않은 콘텐츠를 비즈니스에 바로 사용할 수 있게 만들고, 유용한 정보를 창출합니다.\"\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fac805-7e58-44fd-af24-1c4b878fb2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"인공지능은 일반적으로 인간의 지능이 필요하거나 인간이 분석할 수 있는 것보다 규모가 큰 데이터를 포함하는 방식으로 추론, 학습 및 행동할 수 있는 컴퓨터 및 기계를 구축하는 것과 관련된 과학 분야입니다. AI는 컴퓨터 공학, 데이터 분석 및 통계, 하드웨어 및 소프트웨어 엔지니어링, 언어학, 신경 과학은 물론 철학과 심리학을 포함하여 여러 학문을 포괄하는 광범위한 분야입니다. 비즈니스의 운영 수준에서 AI는 주로 머신러닝과 딥 러닝을 기반으로 하는 기술 모음으로, 데이터 분석, 예상 및 예측, 객체 분류, 자연어 처리, 추천, 지능형 데이터 가져오기 등을 수행할 수 있습니다.\"\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876f1957-a25a-4c3f-8d86-c8ee43ea6a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7661f57c-2a85-4c7c-9339-cfc4ee3119db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
    "from chatgpt.trainer import PPOTrainer\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700a47bb-b1d2-4bb3-b87f-fef8bfe61d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "with NaiveStrategy().model_init_context():\n",
    "    actor = GPTActor(pretrained='models/output_1_SFT', lora_rank=0).to(torch.cuda.current_device())\n",
    "    critic = GPTCritic(pretrained='models/output_2_RM', lora_rank=0).to(torch.cuda.current_device())\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\",\n",
    "        model_max_length=512\n",
    "    )\n",
    "    initial_model = deepcopy(actor)\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465e7fd2-15fa-4b50-a269-9415f510ee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optim = torch.optim.Adam(actor.parameters(), lr=5e-6)\n",
    "critic_optim = torch.optim.Adam(critic.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c7f8cc-a5f5-44ce-92fa-8d415eed45aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = NaiveStrategy().prepare(\n",
    "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958277d7-e0b1-412b-82d7-c49c3561af90",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "\n",
    "def tokenize_fn(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
    "    return {k: v.cuda() for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b35e8de-8572-4e41-9fb8-d1b24626bed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenize_fn('It takes something more than intelligence to act intelligently.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8510b00d-e796-4edc-b521-959f7b209aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fe51f1-94b3-4e34-8f28-3ee5771781c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PPOTrainer(NaiveStrategy(),\n",
    "                     actor,\n",
    "                     critic,\n",
    "                     reward_model,\n",
    "                     initial_model,\n",
    "                     actor_optim,\n",
    "                     critic_optim,\n",
    "                     max_epochs=1,\n",
    "                     train_batch_size=8,\n",
    "                     tokenizer=tokenize_fn,\n",
    "                     max_length=128,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.0,\n",
    "                     top_k=50,\n",
    "                     pad_token_id=tokenizer.pad_token_id,\n",
    "                     eos_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51271d5b-d34b-4db9-a894-b4ace3194ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(list_prompt,\n",
    "            num_episodes=10,\n",
    "            max_timesteps=3,\n",
    "            update_timesteps=3)\n",
    "\n",
    "actor.model.save_pretrained('models/output_3_PPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7616d2-3815-4839-871f-35afdb9469da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation(input_text, model):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    outputs = model.generate(input_ids,\n",
    "                             max_length=250,\n",
    "                             do_sample=True,\n",
    "                             top_k=50,\n",
    "                             top_p=0.95,\n",
    "                             num_return_sequences=1)\n",
    "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    print()\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = [\n",
    "    '불고기용 고기 한우에요?',\n",
    "    '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "    '시카고 오헤어 국제공항은 어디에 있어',\n",
    "    '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "for input_text in list_prompt:\n",
    "    output = generation(input_text, actor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
